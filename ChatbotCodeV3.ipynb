{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-proj-aUqK9ndHY2uRtmibY3HHT3BlbkFJNpEOu1WY50Bz8PuXlWE6\",  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "# %%\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding for a given text using OpenAI's API\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=[text]  # Ensure input is passed as a list\n",
    "    )\n",
    "    # Extract the embedding correctly\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "def load_text_files_and_embed(directory):\n",
    "    \"\"\"Load text files from a directory and compute embeddings\"\"\"\n",
    "    embeddings = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text = text.replace(\"\\n\", \" \")\n",
    "            embedding = generate_embedding(text)\n",
    "            embeddings.append({'id': filename, 'values': embedding, 'metadata': {'text': text}})\n",
    "    return embeddings\n",
    "\n",
    "# %%\n",
    "folder_path = 'D:/hackbangalore/rag_data'\n",
    "data = load_text_files_and_embed(folder_path)\n",
    "\n",
    "# Convert list to DataFrame\n",
    "# add column sparse_values with every values as None\n",
    "for i in range(len(data)):\n",
    "    data[i]['sparse_values'] = None\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "\n",
    "# %%\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=\"f5fa0fda-521f-4e2b-959f-0ba03ea28de3\")\n",
    "\n",
    "# %%\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "\n",
    "# %%\n",
    "index_name = 'hackbangalore-rag-trial-01'\n",
    "\n",
    "# %%\n",
    "import time\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()\n",
    "\n",
    "# %%\n",
    "# convert df to a python list, example:\n",
    "# sample_doc =[\n",
    "#               {\n",
    "#                 \"id\": \"item_0\",\n",
    "#                 \"values\": [\n",
    "#                     0.07446312652229216,\n",
    "#                     0.8866284618893006,\n",
    "#                     0.5244262265711986\n",
    "#                 ],\n",
    "#                 \"metadata\": {\n",
    "#                     \"category\": \"sports\",\n",
    "#                     \"colors\": [\n",
    "#                         \"blue\",\n",
    "#                         \"red\",\n",
    "#                         \"green\"\n",
    "#                     ],\n",
    "#                     \"time_stamp\": 0  }\n",
    "#                }   ]\n",
    "\n",
    "sample_doc = df.to_dict(orient='records')\n",
    "print(sample_doc)\n",
    "index.upsert(sample_doc)\n",
    "\n",
    "# %%\n",
    "import openai\n",
    "\n",
    "# get api key from platform.openai.com\n",
    "openai.api_key = 'sk-proj-aUqK9ndHY2uRtmibY3HHT3BlbkFJNpEOu1WY50Bz8PuXlWE6'\n",
    "\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "# %%\n",
    "query = \"In the sleepy town of Glimmerdale, Benjamin, an amateur clockmaker\"\n",
    "\n",
    "res = client.embeddings.create(\n",
    "    input=[query],\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res.data[0].embedding\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "\n",
    "# %%\n",
    "res\n",
    "\n",
    "# %%\n",
    "# get list of retrieved text\n",
    "contexts = [item['metadata']['text'] for item in res['matches']]\n",
    "\n",
    "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query\n",
    "\n",
    "# %%\n",
    "print(augmented_query)\n",
    "\n",
    "# %%\n",
    "# system message to 'prime' the model\n",
    "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
    "user questions based on the information provided by the user above\n",
    "each question. If the information can not be found in the information\n",
    "provided by the user you truthfully say \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "res = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": primer},\n",
    "        {\"role\": \"user\", \"content\": augmented_query}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# %%\n",
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(res.choices[0].message.content))\n",
    "\n",
    "# %%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import parso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-aUqK9ndHY2uRtmibY3HHT3BlbkFJNpEOu1WY50Bz8PuXlWE6'\n",
    "os.environ['PINECONE_API_KEY'] = 'f5fa0fda-521f-4e2b-959f-0ba03ea28de3'\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "# Pinecone index name\n",
    "index_name = \"hackbangalore-rag-trial-01\"\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "vectorstore = PineconeVectorStore(index_name=index_name,\n",
    "                                  embedding=OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY']))\n",
    "\n",
    "\n",
    "def extract_keywords_using_chatgpt(text):\n",
    "    \"\"\"\n",
    "    Sends a request to ChatGPT to extract keywords from the provided text.\n",
    "\n",
    "    :param text: The text from which keywords need to be extracted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Invoke the LLM to get the response, which is expected to be directly the keyword string\n",
    "        response = llm.invoke(\n",
    "            \"Extract the main keywords from the following text and provide them as a comma-separated list. do not give more than 10 keywords Please prefix the keywords with 'Keywords:'. the text to extract keyword is, \" + text\n",
    "        )\n",
    "        print(\"Response from ChatGPT:\", response)  # Debug: Print the full response to verify its structure\n",
    "        print(\"Type of response:\", type(response))\n",
    "        print(\"Content of response:\", response)\n",
    "\n",
    "        # Check if the response is directly usable\n",
    "        # strip /n and /r from the response\n",
    "        response = response.replace('\\n', '').replace('\\r', '')\n",
    "        if isinstance(response, str) and response.startswith('Keywords:'):\n",
    "            print(\"Response is directly usable\")\n",
    "            keywords = response[len('Keywords:'):].strip()\n",
    "            return [keyword.strip() for keyword in keywords.split(',')]\n",
    "        elif isinstance(response, dict) and 'text' in response:\n",
    "            output = response['text'].strip()\n",
    "            if output.startswith('Keywords:'):\n",
    "                keywords = output[len('Keywords:'):].strip()\n",
    "                return [keyword.strip() for keyword in keywords.split(',')]\n",
    "        else:\n",
    "            print(\"Unexpected response structure or no keywords prefix found\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "@app.route('/search_projects', methods=['POST'])\n",
    "def search_projects():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        description = data.get('description', '')\n",
    "\n",
    "        # Extract keywords relevant to the description\n",
    "        keywords = extract_keywords_using_chatgpt(description)\n",
    "\n",
    "        # Perform a similarity search using the vector store with extracted keywords\n",
    "        search_query = ' '.join(keywords)\n",
    "        print(f\"Search query: {search_query}\")\n",
    "        search_result = vectorstore.similarity_search(search_query)\n",
    "\n",
    "        # Return the top 3 results, if more than 3 exist\n",
    "        top_results = search_result[:3] if len(search_result) > 3 else search_result\n",
    "\n",
    "        if top_results:\n",
    "            response_data = [{\"similarity_match\": result.page_content} for result in top_results]\n",
    "            return jsonify(response_data), 200\n",
    "        else:\n",
    "            return jsonify({'message': 'No results found'}), 404\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
